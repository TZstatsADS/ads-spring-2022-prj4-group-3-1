{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb9c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import scipy.optimize as optim\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "from IPython.display import Markdown, display\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "from tabulate import tabulate\n",
    "import sys\n",
    "from LFR import *\n",
    "from LFR import distances\n",
    "\n",
    "# this function defines the threshold for y_n_hat to be 0 or 1\n",
    "def predic_category(y):\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 0.5:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return y\n",
    "\n",
    "# this function calculate y_n_hat by using the best parameters\n",
    "def predict(params, data_sensitive, data_nonsensitive, k=10):\n",
    "    \n",
    "    Ns, P = data_sensitive.shape\n",
    "    Nns, _ = data_nonsensitive.shape\n",
    "    \n",
    "    # form parameters in new forms\n",
    "    alpha0 = params[:P]\n",
    "    alpha1 = params[P : 2 * P]\n",
    "    w = params[2 * P : (2 * P) + k]\n",
    "    v = np.matrix(params[(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    dists_sensitive = distances(data_sensitive, v, alpha0, Ns, P, k)\n",
    "    dists_nonsensitive = distances(data_nonsensitive, v, alpha1, Nns, P, k)\n",
    "\n",
    "    M_nk_sensitive = M_nk(dists_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive = M_nk(dists_nonsensitive, Nns, k)\n",
    "    \n",
    "    M_k_sensitive = M_k(M_nk_sensitive, Ns, k)\n",
    "    M_k_nonsensitive = M_k(M_nk_nonsensitive, Nns, k)\n",
    "    \n",
    "    # make predictions for sensitive data\n",
    "    yhat_sensitive = np.zeros(Ns)\n",
    "    for i in range(Ns):\n",
    "        for j in range(k):\n",
    "            yhat_sensitive[i] += M_nk_sensitive[i, j] * w[j]\n",
    "        yhat_sensitive[i] = 1e-6 if yhat_sensitive[i] <= 0 else yhat_sensitive[i]\n",
    "        yhat_sensitive[i] = 0.999 if yhat_sensitive[i] >= 1 else yhat_sensitive[i]\n",
    "        \n",
    "    # make predictions for nonsensitive data\n",
    "    yhat_nonsensitive = np.zeros(Nns)\n",
    "    for i in range(Nns):\n",
    "        for j in range(k):\n",
    "            yhat_nonsensitive[i] += M_nk_nonsensitive[i, j] * w[j]\n",
    "        yhat_nonsensitive[i] = 1e-6 if yhat_nonsensitive[i] <= 0 else yhat_nonsensitive[i]\n",
    "        yhat_nonsensitive[i] = 0.999 if yhat_nonsensitive[i] >= 1 else yhat_nonsensitive[i]\n",
    "        \n",
    "    final_y_s = predic_category(yhat_sensitive)\n",
    "    final_y_n = predic_category(yhat_nonsensitive)\n",
    "    \n",
    "    return final_y_s, final_y_n\n",
    "\n",
    "def calc_accuracy(y_sen, y_nsen, y_sen_label, y_nsen_label):\n",
    "    y_sen_df = pd.DataFrame(y_sen)\n",
    "    y_nsen_df = pd.DataFrame(y_nsen)\n",
    "    y_label = pd.DataFrame(y_sen_label).append(pd.DataFrame(y_nsen_label))\n",
    "    y_df = y_sen_df.append(y_nsen_df)\n",
    "    \n",
    "    acc_sen = accuracy_score(y_sen_label, y_sen_df)\n",
    "    acc_nsen = accuracy_score(y_nsen_label, y_nsen_df)\n",
    "    total_accuracy = accuracy_score(y_label, y_df)\n",
    "    \n",
    "    return acc_sen, acc_nsen, total_accuracy\n",
    "\n",
    "def calc_calibration(acc_sen, acc_nsen):\n",
    "    return abs(acc_sen - acc_nsen)\n",
    "\n",
    "def get_model_performance(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, matrix, f1\n",
    "\n",
    "def plot_model_performance(y_pred_s, y_pred_n, y_pred, y_true_s, y_true_n, y_true):\n",
    "    accuracy_s, matrix_s, f1_s = get_model_performance(y_true_s, y_pred_s)\n",
    "\n",
    "    display(Markdown('#### Sensitive data (Caucasians):'))\n",
    "    print(f'Accuracy: {accuracy_s}')\n",
    "    print(f'F1 score: {f1_s}')\n",
    "    \n",
    "    accuracy_n, matrix_n, f1_n = get_model_performance(y_true_n, y_pred_n)\n",
    "\n",
    "    display(Markdown('#### Nonsensitive data (African-Americans):'))\n",
    "    print(f'Accuracy: {accuracy_n}')\n",
    "    print(f'F1 score: {f1_n}')\n",
    "    \n",
    "    accuracy, matrix, f1 = get_model_performance(y_true, y_pred)\n",
    "\n",
    "    display(Markdown('#### All data:'))\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'F1 score: {f1}')\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    sns.heatmap(matrix_s, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title('Confusion Matrix (sensitive data)')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    sns.heatmap(matrix_n, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title('Confusion Matrix (nonsensitive data)')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title('Confusion Matrix (all data)')\n",
    "    \n",
    "def equal_opportunity_difference(y_test_s, y_test_n, pred_test_s, pred_test_n):\n",
    "    tpr_s = 0\n",
    "    for i in range(len(y_test_s)):\n",
    "        if y_test_s[i] == 1 and pred_test_s[i] == 1:\n",
    "            tpr_s += 1 \n",
    "    tpr_s = tpr_s/len(y_test_s)\n",
    "    tpr_n = 0\n",
    "    for i in range(len(y_test_n)):\n",
    "        if y_test_n[i] == 1 and pred_test_n[i] == 1:\n",
    "            tpr_n += 1 \n",
    "    tpr_n = tpr_n/len(y_test_n)\n",
    "    \n",
    "    equal_opportunity_difference = tpr_s - tpr_n\n",
    "    \n",
    "    return equal_opportunity_difference\n",
    "\n",
    "def avg_abs_odds_difference(y_test_s, y_test_n, pred_test_s, pred_test_n):\n",
    "    tpr_s = 0\n",
    "    for i in range(len(y_test_s)):\n",
    "        if y_test_s[i] == 1 and pred_test_s[i] == 1:\n",
    "            tpr_s += 1 \n",
    "    tpr_s = tpr_s/len(y_test_s)\n",
    "    tpr_n = 0\n",
    "    for i in range(len(y_test_n)):\n",
    "        if y_test_n[i] == 1 and pred_test_n[i] == 1:\n",
    "            tpr_n += 1 \n",
    "    tpr_n = tpr_n/len(y_test_n)\n",
    "    \n",
    "    fpr_s = 0\n",
    "    for i in range(len(y_test_s)):\n",
    "        if y_test_s[i] == 0 and pred_test_s[i] == 1:\n",
    "            fpr_s += 1 \n",
    "    fpr_s = fpr_s/len(y_test_s)\n",
    "    fpr_n = 0\n",
    "    for i in range(len(y_test_n)):\n",
    "        if y_test_n[i] == 0 and pred_test_n[i] == 1:\n",
    "            fpr_n += 1 \n",
    "    fpr_n = fpr_n/len(y_test_n)\n",
    "    \n",
    "    avg_abs_odds_diff = 0.5*(abs(fpr_s - fpr_n) + abs(tpr_s - tpr_n))\n",
    "    \n",
    "    return avg_abs_odds_diff\n",
    "\n",
    "def fair_metrics(pred_test_s, pred_test_n, pred_test, y_test_s, y_test_n, y_test):\n",
    "    \n",
    "    cols = ['calibration', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact']\n",
    "    obj_fairness = [[0,0,0,1]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    acc_sen, acc_nsen, total_accuracy = calc_accuracy(pred_test_s, pred_test_n, y_test_s, y_test_n)\n",
    "    \n",
    "    calibration = acc_sen - acc_nsen\n",
    "    \n",
    "    equal_opp_diff = equal_opportunity_difference(y_test_s, y_test_n, pred_test_s, pred_test_n)\n",
    "    \n",
    "    avg_abs_odds_diff = avg_abs_odds_difference(y_test_s, y_test_n, pred_test_s, pred_test_n)\n",
    "    \n",
    "    disparate_impact = acc_sen/acc_nsen\n",
    "    \n",
    "    row = pd.DataFrame([[calibration, equal_opp_diff, avg_abs_odds_diff, disparate_impact]],\n",
    "                           columns  = cols,\n",
    "                           index = ['Race']\n",
    "                          )\n",
    "    \n",
    "    fair_metrics = fair_metrics.append(row)\n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "    \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8]\n",
    "    bottom = [-1,-1,-1,0]\n",
    "    top = [1,1,1,2]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these four metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,4)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 4 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,4):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/4, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')\n",
    "\n",
    "def compare_models(pred_1_test_s, pred_1_test_n, pred_2_test_s, pred_2_test_n, y_test_s, y_test_n, \n",
    "                  fair_metrics_1, fair_metrics_2, model1, model2):\n",
    "    acc_1_sen, acc_1_nsen, total_accuracy_1 = calc_accuracy(pred_1_test_s, pred_1_test_n, y_test_s, y_test_n)\n",
    "    acc_2_sen, acc_2_nsen, total_accuracy_2 = calc_accuracy(pred_2_test_s, pred_2_test_n, y_test_s, y_test_n)\n",
    "\n",
    "    calibration_1 = fair_metrics_1.iloc[1]['calibration']\n",
    "    equal_opp_diff_1 = fair_metrics_1.iloc[1]['equal_opportunity_difference']\n",
    "    avg_abs_odds_diff_1 = fair_metrics_1.iloc[1]['average_abs_odds_difference']\n",
    "    disparate_impact_1 = fair_metrics_1.iloc[1]['disparate_impact']\n",
    "\n",
    "    calibration_2 = fair_metrics_2.iloc[1]['calibration']\n",
    "    equal_opp_diff_2 = fair_metrics_2.iloc[1]['equal_opportunity_difference']\n",
    "    avg_abs_odds_diff_2 = fair_metrics_2.iloc[1]['average_abs_odds_difference']\n",
    "    disparate_impact_2 = fair_metrics_2.iloc[1]['disparate_impact']\n",
    "    \n",
    "    print(tabulate([['accuracy', total_accuracy_1, total_accuracy_2], \n",
    "                ['calibration', calibration_1, calibration_2],\n",
    "                ['equal_opportunity_difference', equal_opp_diff_1, equal_opp_diff_2],\n",
    "                ['average_abs_odds_difference', avg_abs_odds_diff_1, avg_abs_odds_diff_2],\n",
    "                ['disparate_impact', disparate_impact_1, disparate_impact_2]], headers=['metric', model1, model2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1e8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
